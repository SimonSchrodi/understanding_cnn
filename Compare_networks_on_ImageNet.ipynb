{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compare networks on ImageNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/infomon/understanding_cnn/blob/master/Compare_networks_on_ImageNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "DqkiiG7KSJVD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Compare networks on ImageNet"
      ]
    },
    {
      "metadata": {
        "id": "lVTelsQGSJVL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook we show how one can use **iNNvestigate** to analyze the prediction of *different* ImageNet-models!\n",
        "\n",
        "This notebook is an extension of the [Comparing networks on ImagenNet](imagenet_network_comparison.ipynb) notebook.\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "Note this script is fairly slow, because we are rebuilding models times analysis methods many computational graphs.\n",
        "\n",
        "-----\n",
        "\n",
        "**To use this notebook please download the example images using the following script:**\n",
        "\n",
        "`innvestigate/examples/images/wget_imagenet_2011_samples.sh`"
      ]
    },
    {
      "metadata": {
        "id": "gCSz5J_iSJVQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "NFCz2fStSNX2",
        "colab_type": "code",
        "outputId": "75c6bd4b-6a64-42c7-a71c-a8abd9bdd157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/albermax/innvestigate\n",
        "!pip install -q deeplift\n",
        "!git clone https://github.com/infomon/understanding_cnn\n",
        "  \n",
        "import shutil\n",
        "import os\n",
        "if not os.path.isfile(\"utils.py\"):\n",
        "  shutil.move(\"/content/understanding_cnn/utils/utils.py\", \"/content\")\n",
        "if not os.path.isfile(\"utils_imagenet.py\"):\n",
        "  shutil.move(\"/content/understanding_cnn/utils/utils_imagenet.py\", \"/content\")\n",
        "if not os.path.isdir(\"models\"):\n",
        "  shutil.move(\"/content/understanding_cnn/models\", \"/content\")\n",
        "if not os.path.isdir(\"images\"):\n",
        "  shutil.move(\"/content/understanding_cnn/data/images\", \"/content\")\n",
        "if not os.path.isfile(\"data_loader.py\"):\n",
        "  shutil.move(\"/content/understanding_cnn/data/data_loader.py\", \"/content\")\n",
        "  \n",
        "!rm -r understanding_cnn\n",
        "\n",
        "!pip install scipy==1.2.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for innvestigate (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCloning into 'understanding_cnn'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 102 (delta 36), reused 72 (delta 15), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (102/102), 14.81 MiB | 25.74 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n",
            "Requirement already satisfied: scipy==1.2.0 in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.0) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FYmqnPrvSJVV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvP2z9aeSJVk",
        "colab_type": "code",
        "outputId": "2ff526bc-9f64-4245-da34-8e8daf251416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline  \n",
        "\n",
        "import imp\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import keras\n",
        "import keras.backend\n",
        "import keras.models\n",
        "\n",
        "import innvestigate\n",
        "import innvestigate.applications.imagenet\n",
        "import innvestigate.utils as iutils\n",
        "\n",
        "import models.model_loader as model_loader\n",
        "import data_loader\n",
        "\n",
        "# Use utility libraries to focus on relevant iNNvestigate routines.\n",
        "eutils = imp.load_source(\"utils\", \"utils.py\")\n",
        "imgnetutils = imp.load_source(\"utils_imagenet\", \"utils_imagenet.py\")\n",
        "\n",
        "# We create many graphs, let's not run out of memory.\n",
        "if keras.backend.backend() == \"tensorflow\":\n",
        "    config = keras.backend.tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    keras.backend.set_session(keras.backend.tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0319 21:56:41.258181 140400163059584 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "RFbx3_V-SJV-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models, data and analyzers"
      ]
    },
    {
      "metadata": {
        "id": "jKt7AuA1SJWC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We choose a set of ImageNet models:"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "RelTe1AQSJWI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Choose a list of models\n",
        "netnames = [\n",
        "    # NAME                  MODEL LOADER\n",
        "    #[\"AlexNet\",             model_loader.AlexNet],\n",
        "    [\"VGG19\",               model_loader.VGG19],\n",
        "    [\"Inception_v3\",        model_loader.Inception_v3],\n",
        "    [\"Inception_Resnet_v2\", model_loader.Inception_Resnet_v2],\n",
        "    [\"Resnet_v2_152\",       model_loader.Resnet_v2_152],\n",
        "    [\"ResNeXt_101\",         model_loader.Resnet_v1_101]\n",
        "]          \n",
        "n_nets = len(netnames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NIhz5BB7SJWY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following function will load a specific model, load the data in the respective format and create analyzers for this model.\n",
        "\n",
        "**For a better understanding of this part we refer to the [Comparing networks on ImagenNet](imagenet_network_comparison.ipynb) notebook, from which this code segment is adopted from.**"
      ]
    },
    {
      "metadata": {
        "id": "TgXdS-dMSJWe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_model_data_and_analyzers(loader):\n",
        "    # Load the model definition.\n",
        "    model = loader()\n",
        "\n",
        "    # Get some example test set images.\n",
        "    data = data_loader.load_from_folder(\"images\",model.get_image_size())\n",
        "\n",
        "    \n",
        "    images = [] \n",
        "    label_to_class_name = []\n",
        "    for img,label in data:\n",
        "      images.append(img)\n",
        "      label_to_class_name.append(label)\n",
        "      \n",
        "    \n",
        "    patterns = model.get_patterns()\n",
        "    input_range = (-1,1)\n",
        "\n",
        "    noise_scale = (input_range[1]-input_range[0]) * 0.1\n",
        "\n",
        "    # Methods we use and some properties.\n",
        "    methods = [\n",
        "        # NAME                    OPT.PARAMS                POSTPROC FXN                TITLE\n",
        "        # Show input.\n",
        "        (\"input\",                 {},                       imgnetutils.image,         \"Input\"), #0\n",
        "\n",
        "        # Function\n",
        "        (\"gradient\",              {\"postprocess\": \"abs\"},   imgnetutils.graymap,       \"Gradient\"), #1\n",
        "        (\"smoothgrad\",            {\"augment_by_n\": 16,\n",
        "                                   \"noise_scale\": noise_scale,\n",
        "                                   \"postprocess\": \"square\"},imgnetutils.graymap,       \"SmoothGrad\"), #2\n",
        "\n",
        "        # Signal\n",
        "        (\"deconvnet\",             {},                       imgnetutils.bk_proj,       \"Deconvnet\"), #3\n",
        "        (\"guided_backprop\",       {},                       imgnetutils.bk_proj,       \"Guided Backprop\",), #4\n",
        "        (\"pattern.net\",           {\"patterns\": patterns},   imgnetutils.bk_proj,       \"PatternNet\"), #5\n",
        "\n",
        "        # Interaction\n",
        "        (\"pattern.attribution\",   {\"patterns\": patterns},   imgnetutils.heatmap,       \"PatternAttribution\"), #6\n",
        "        (\"deep_taylor.bounded\",   {\"low\": input_range[0],\n",
        "                                   \"high\": input_range[1]}, imgnetutils.heatmap,       \"DeepTaylor\"), #7\n",
        "        (\"input_t_gradient\",      {},                       imgnetutils.heatmap,       \"Input * Gradient\"), #8\n",
        "        (\"integrated_gradients\",  {\"reference_inputs\": input_range[0],\n",
        "                                   \"steps\": 16},            imgnetutils.heatmap,       \"Integrated Gradients\"), #9\n",
        "        (\"lrp.epsilon\",           {\"epsilon\": 1},           imgnetutils.heatmap,        \"LRP-Epsilon\"), #10\n",
        "        (\"lrp.epsilon_IB\",           {\"epsilon\": 1},           imgnetutils.heatmap,        \"LRP-Epsilon\"), #11\n",
        "        (\"lrp.alpha_1_beta_0\",           {},           imgnetutils.heatmap,        \"LRP-Alpha1-Beta0\"), #12\n",
        "        (\"lrp.alpha_1_beta_0_IB\",           {},           imgnetutils.heatmap,        \"LRP-Alpha1-Beta0 IB\"), #13\n",
        "        (\"lrp.alpha_2_beta_1\",           {},           imgnetutils.heatmap,        \"LRP-Alpha2-Beta1\"), #14\n",
        "        (\"lrp.alpha_2_beta_1_IB\",           {},           imgnetutils.heatmap,        \"LRP-Alpha2-Beta1 IB\"), #15\n",
        "        (\"lrp.sequential_preset_a_flat\",{\"epsilon\": 1},     imgnetutils.heatmap,       \"LRP-PresetAFlat\"), #16 \n",
        "        (\"lrp.sequential_preset_b_flat\",{\"epsilon\": 1},     imgnetutils.heatmap,       \"LRP-PresetBFlat\"), #17\n",
        "    ]\n",
        "    \n",
        "    # Select methods of your choice\n",
        "    selected_methods_indices = [0,1,11,13,15]\n",
        "    selected_methods = [methods[i] for i in selected_methods_indices]\n",
        "    print('Using method(s) \"{}\".'.format([method[0] for method in selected_methods]))\n",
        "    \n",
        "    # Create model without trailing softmax\n",
        "    model_wo_softmax = model.get_model()\n",
        "\n",
        "    # Create analyzers.\n",
        "    analyzers = []\n",
        "    for method in selected_methods:\n",
        "        try:\n",
        "            analyzer = innvestigate.create_analyzer(method[0],        # analysis method identifier\n",
        "                                                    model_wo_softmax, # model without softmax output\n",
        "                                                    **method[1])      # optional analysis parameters\n",
        "        except innvestigate.NotAnalyzeableModelException:\n",
        "            # Not all methods work with all models.\n",
        "            analyzer = None\n",
        "            print(method[3]+\" cannot be used!\")\n",
        "        analyzers.append(analyzer)\n",
        "        \n",
        "    return (images, label_to_class_name,\n",
        "            selected_methods, model, model_wo_softmax, analyzers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AtwwGzixSJWt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ]
    },
    {
      "metadata": {
        "id": "1XaJHM3HSJWy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we analyze each image with the different networks and different analyzers:"
      ]
    },
    {
      "metadata": {
        "id": "c4iOXexXSJW3",
        "colab_type": "code",
        "outputId": "595a69f9-3351-4bde-d772-ac763069608f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4277
        }
      },
      "cell_type": "code",
      "source": [
        "analyses = {}\n",
        "texts = {}\n",
        "    \n",
        "for (netname,loader) in netnames:\n",
        "    print(\"Creating analyses for network {}.\".format(netname))\n",
        "    tmp = prepare_model_data_and_analyzers(loader)\n",
        "    (images, label_to_class_name,\n",
        "     methods, model, model_wo_softmax, analyzers) = tmp\n",
        "    \n",
        "    analysis = np.zeros([len(images), len(analyzers)]+list(model.get_image_size())+[3])\n",
        "    text = []\n",
        "    \n",
        "    channels_first = keras.backend.image_data_format() == \"channels_first\"\n",
        "    color_conversion = \"BGRtoRGB\" if model.get_color_coding() == \"BGR\" else None\n",
        "\n",
        "    for i, x in enumerate(images):\n",
        "        # Add batch axis.\n",
        "        x_pp = model.preprocess_input(x)\n",
        "\n",
        "        # Predict final activations, probabilites, and label.\n",
        "        presm = model.predict_wo_softmax(x_pp)[0]\n",
        "        prob = model.predict_with_softmax(x_pp)[0]\n",
        "        y_hat = prob.argmax()\n",
        "\n",
        "        # Save prediction info:\n",
        "        text.append((\"%s\" % label_to_class_name[i],    # ground truth label\n",
        "                     \"%.2f\" % presm.max(),             # pre-softmax logits\n",
        "                     \"%.2f\" % prob.max(),              # probabilistic softmax output  \n",
        "                     \"%s\" % model.decode_predictions(prob[None,...], top=1)[0] # predicted label\n",
        "                    ))\n",
        "\n",
        "        for aidx, analyzer in enumerate(analyzers):\n",
        "            print(methods[aidx][0]+\" for image with label \"+label_to_class_name[i])\n",
        "            if methods[aidx][0] == \"input\":\n",
        "                # Do not analyze, but keep not preprocessed input.\n",
        "                a = x / 255\n",
        "            elif analyzer:\n",
        "                # Analyze.\n",
        "                a = analyzer.analyze(x_pp)\n",
        "\n",
        "                # Apply common postprocessing, e.g., re-ordering the channels for plotting.\n",
        "                a = imgnetutils.postprocess(a, color_conversion, channels_first)\n",
        "                # Apply analysis postprocessing, e.g., creating a heatmap.\n",
        "                a = methods[aidx][2](a)\n",
        "            else:\n",
        "                a = np.zeros_like(x)\n",
        "            # Store the analysis.\n",
        "            analysis[i, aidx] = a[0]\n",
        "\n",
        "        analyses[netname] = analysis\n",
        "        texts[netname] = text"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating analyses for network VGG19.\n",
            "Using method(s) \"['input', 'gradient', 'lrp.epsilon_IB', 'lrp.alpha_1_beta_0_IB', 'lrp.alpha_2_beta_1_IB']\".\n",
            "input for image with label dog\n",
            "gradient for image with label dog\n",
            "lrp.epsilon_IB for image with label dog\n",
            "lrp.alpha_1_beta_0_IB for image with label dog\n",
            "lrp.alpha_2_beta_1_IB for image with label dog\n",
            "input for image with label bell pepper\n",
            "gradient for image with label bell pepper\n",
            "lrp.epsilon_IB for image with label bell pepper\n",
            "lrp.alpha_1_beta_0_IB for image with label bell pepper\n",
            "lrp.alpha_2_beta_1_IB for image with label bell pepper\n",
            "input for image with label airplane\n",
            "gradient for image with label airplane\n",
            "lrp.epsilon_IB for image with label airplane\n",
            "lrp.alpha_1_beta_0_IB for image with label airplane\n",
            "lrp.alpha_2_beta_1_IB for image with label airplane\n",
            "input for image with label leopard\n",
            "gradient for image with label leopard\n",
            "lrp.epsilon_IB for image with label leopard\n",
            "lrp.alpha_1_beta_0_IB for image with label leopard\n",
            "lrp.alpha_2_beta_1_IB for image with label leopard\n",
            "input for image with label husky\n",
            "gradient for image with label husky\n",
            "lrp.epsilon_IB for image with label husky\n",
            "lrp.alpha_1_beta_0_IB for image with label husky\n",
            "lrp.alpha_2_beta_1_IB for image with label husky\n",
            "input for image with label tabby_cat\n",
            "gradient for image with label tabby_cat\n",
            "lrp.epsilon_IB for image with label tabby_cat\n",
            "lrp.alpha_1_beta_0_IB for image with label tabby_cat\n",
            "lrp.alpha_2_beta_1_IB for image with label tabby_cat\n",
            "input for image with label gruffed grouse\n",
            "gradient for image with label gruffed grouse\n",
            "lrp.epsilon_IB for image with label gruffed grouse\n",
            "lrp.alpha_1_beta_0_IB for image with label gruffed grouse\n",
            "lrp.alpha_2_beta_1_IB for image with label gruffed grouse\n",
            "input for image with label accordion\n",
            "gradient for image with label accordion\n",
            "lrp.epsilon_IB for image with label accordion\n",
            "lrp.alpha_1_beta_0_IB for image with label accordion\n",
            "lrp.alpha_2_beta_1_IB for image with label accordion\n",
            "input for image with label african elephant\n",
            "gradient for image with label african elephant\n",
            "lrp.epsilon_IB for image with label african elephant\n",
            "lrp.alpha_1_beta_0_IB for image with label african elephant\n",
            "lrp.alpha_2_beta_1_IB for image with label african elephant\n",
            "input for image with label subway train\n",
            "gradient for image with label subway train\n",
            "lrp.epsilon_IB for image with label subway train\n",
            "lrp.alpha_1_beta_0_IB for image with label subway train\n",
            "lrp.alpha_2_beta_1_IB for image with label subway train\n",
            "Creating analyses for network Inception_v3.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0319 22:01:03.681388 140400163059584 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0319 22:01:15.967624 140400163059584 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_344 (Lambda)          (None, 1001)              0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node random_uniform_186/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-002061fd4325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnetname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnetnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating analyses for network {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_model_data_and_analyzers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     (images, label_to_class_name,\n\u001b[1;32m      8\u001b[0m      methods, model, model_wo_softmax, analyzers) = tmp\n",
            "\u001b[0;32m<ipython-input-8-95af3abf2906>\u001b[0m in \u001b[0;36mprepare_model_data_and_analyzers\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_model_data_and_analyzers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Load the model definition.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get some example test set images.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/models/model_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classifier_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hub_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__classifier_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/models/model_utils.py\u001b[0m in \u001b[0;36mget_hub_model\u001b[0;34m(classifier_url)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node random_uniform_186/RandomUniform (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4139) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'random_uniform_186/RandomUniform', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-002061fd4325>\", line 40, in <module>\n    a = analyzer.analyze(x_pp)\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py\", line 473, in analyze\n    self.create_analyzer_model()\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py\", line 411, in create_analyzer_model\n    model, stop_analysis_at_tensors=stop_analysis_at_tensors)\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/relevance_based/relevance_analyzer.py\", line 453, in _create_analysis\n    return super(LRP, self)._create_analysis(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py\", line 711, in _create_analysis\n    return_all_reversed_tensors=return_all_reversed_tensors)\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py\", line 700, in _reverse_model\n    return_all_reversed_tensors=return_all_reversed_tensors)\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/utils/keras/graph.py\", line 974, in reverse_model\n    \"layer\": layer,\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/relevance_based/relevance_analyzer.py\", line 419, in create_rule_mapping\n    rule = rule_class(layer, reverse_state)\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/relevance_based/relevance_analyzer.py\", line 600, in __init__\n    **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/relevance_based/relevance_rule.py\", line 262, in __init__\n    name_template=\"reversed_kernel_positive_%s\")\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/utils/keras/graph.py\", line 320, in copy_layer_wo_activation\n    return get_layer_from_config(layer, config, weights=weights, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/innvestigate/utils/keras/graph.py\", line 268, in get_layer_from_config\n    new_layer.build(input_shapes)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\", line 866, in build\n    constraint=self.kernel_constraint)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 249, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/usr/local/lib/python3.6/dist-packages/keras/initializers.py\", line 218, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 4139, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\", line 247, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_random_ops.py\", line 777, in random_uniform\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node random_uniform_186/RandomUniform (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4139) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "E2auNo1OSJXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we visualize the analysis results:"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "YxSYIxbFSJXV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_images = analyses[netnames[0]].shape[0]\n",
        "\n",
        "# Prepare common labels\n",
        "col_labels = [''.join(method[3]) for method in methods]\n",
        "\n",
        "for image_index in range(n_images):\n",
        "    grid = []\n",
        "    row_labels_left = []\n",
        "    row_labels_right = []\n",
        "    \n",
        "    for netname in netnames:\n",
        "        analysis, text = analyses[netname], texts[netname]\n",
        "        # Prepare the grid as rectengular list\n",
        "        grid.append([analysis[image_index, j] for j in range(analysis.shape[1])])\n",
        "        # Prepare the labels\n",
        "        label, presm, prob, pred = zip(*text)\n",
        "        label = label[image_index]\n",
        "        row_labels_left.append(('network: {}'.format(netname),'pred: {}'.format(pred[image_index])))\n",
        "        row_labels_right.append(('logit: {}'.format(presm[image_index]),'prob: {}'.format(prob[image_index])))\n",
        "\n",
        "    # Plot the analysis.\n",
        "    print(\"Image nr. {}, true label: {}\".format(image_index, label))\n",
        "    eutils.plot_image_grid(grid, row_labels_left, row_labels_right, col_labels,\n",
        "                           file_name=os.environ.get(\"plot_file_name\", None))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "quYd1UlzSJXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This figures show the analysis regarding the *actually predicted* class as computed by the selected analyzers. Each column shows the visualized results for different analyzers and each row shows the analyses wrt to one input sample. To the left of each row, the ground truth label `label` and the predicted label `pred` are show. To the right, the model's probabilistic (softmax) output is shown as `prob` and the logit output just before the terminating softmax layer as `logit`. Note that all analyses have been performed based on the logit output (layer)."
      ]
    }
  ]
}