{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Continuity Explanation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/infomon/understanding_cnn/blob/master/Continuity_Explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "65QbuEEk5cek",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation of Analyzers using\n",
        "This notebook will provide an example how to evaluate analyzers via translation of images.\n",
        "\n",
        "The notebook is based on the following axiom:\n",
        "Assuming the prediction function $f(x)$ is continious, then if two data points are nearly equivalent, then the explanations of their predictions should also be nearly equivalent.([Montavon et. al., 2018](https://reader.elsevier.com/reader/sd/pii/S1051200417302385?token=56EDF10BC4B2EE334D439DCD801F981B5B6953A2F821EA0CB38175B370AFF0BD8689F4CDD8411FF2E2D9CC997C533C46))\n",
        "Explanation continuity (or lack of it) can be quantified by looking for the strongest variation of the explanation $R(x)$ in the input domain. This is called SVE (**S**trongest **V**ariation of **E**xplanation)\n",
        "\n",
        "\\begin{equation}\n",
        "SVE=\\max_{x \\neq x'} \\frac{\\lVert R(x) - R(x') \\lVert_1}{\\lVert x - x' \\lVert_2}\n",
        "\\end{equation}\n",
        "\n",
        "If the technique satisfies explanation continuity, the explanations produced also must change continuously without sudden jumps.\n",
        "Also a lower SVE indicates that the continuity holds for the given analyzer.\n",
        "\n",
        "For a qualitative analysis a input image is translated horizontally from left to right. Then, for every translated image the prediction is saved and afterwards plotted into a diagram.\n",
        "\n",
        "The analyzers used are based on the implementation of the [iNNvestigate Toolbox](https://arxiv.org/abs/1808.04260)"
      ]
    },
    {
      "metadata": {
        "id": "QLvBI_yu9wR6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install iNNvestigate Toolbox"
      ]
    },
    {
      "metadata": {
        "id": "vdnY3Fp65QTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "outputId": "17c34a01-9c7a-438e-9d3b-5d4ced79856e"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!git clone https://github.com/infomon/innvestigate\n",
        "!cd innvestigate\n",
        "!pip install -e innvestigate\n",
        "!cd ..\n",
        "!rm -r innvestigate\n",
        "import innvestigate\n",
        "import innvestigate.utils as iutils"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'innvestigate'...\n",
            "remote: Enumerating objects: 350, done.\u001b[K\n",
            "remote: Counting objects: 100% (350/350), done.\u001b[K\n",
            "remote: Compressing objects: 100% (166/166), done.\u001b[K\n",
            "remote: Total 4493 (delta 206), reused 319 (delta 184), pack-reused 4143\u001b[K\n",
            "Receiving objects: 100% (4493/4493), 28.51 MiB | 22.86 MiB/s, done.\n",
            "Resolving deltas: 100% (3103/3103), done.\n",
            "Obtaining file:///content/innvestigate\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from innvestigate==1.0.7) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from innvestigate==1.0.7) (2.8.0)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (from innvestigate==1.0.7) (2.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from innvestigate==1.0.7) (1.14.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from innvestigate==1.0.7) (4.1.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from innvestigate==1.0.7) (3.6.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from innvestigate==1.0.7) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->innvestigate==1.0.7) (1.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->innvestigate==1.0.7) (1.0.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->innvestigate==1.0.7) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->innvestigate==1.0.7) (1.0.9)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->innvestigate==1.0.7) (0.46)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate==1.0.7) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate==1.0.7) (40.8.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate==1.0.7) (1.8.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate==1.0.7) (19.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate==1.0.7) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate==1.0.7) (6.0.0)\n",
            "Installing collected packages: innvestigate\n",
            "  Found existing installation: innvestigate 1.0.7\n",
            "    Can't uninstall 'innvestigate'. No files were found to uninstall.\n",
            "  Running setup.py develop for innvestigate\n",
            "Successfully installed innvestigate\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2203b9f76eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm -r innvestigate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minnvestigate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0minnvestigate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'innvestigate.utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "g5gP3QsNANP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c756540d-f2a4-46fa-8c57-f30211e27031"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q deeplift\n",
        "!git clone https://github.com/infomon/understanding_cnn\n",
        "import shutil\n",
        "shutil.move(\"/content/understanding_cnn/utils/utils.py\", \"/content\")\n",
        "shutil.move(\"/content/understanding_cnn/utils/utils_mnist.py\", \"/content\")\n",
        "shutil.move(\"/content/understanding_cnn/models/pretrained_models/MNISTcnn.h5\", \"/content\")\n",
        "!rm -r understanding_cnn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deeplift (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCloning into 'understanding_cnn'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 61 (delta 16), reused 42 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (61/61), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IM97R9MC-KbJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ]
    },
    {
      "metadata": {
        "id": "7B16w01g-MZq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EVnJRFZj-Tiz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "e8455fce-4e2a-4e4a-c596-ecfc125cbfd6"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline  \n",
        "\n",
        "import imp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "import innvestigate\n",
        "import innvestigate.utils as iutils\n",
        "\n",
        "mnistutils = imp.load_source(\"utils_mnist\", \"utils_mnist.py\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-08a8e19d48e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minnvestigate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0minnvestigate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmnistutils\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utils_mnist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"utils_mnist.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'innvestigate.utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "poeDzK4C-seI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "Then, the MNIST data is loaded in its entirety, formatted according to the specifications of the Keras backend."
      ]
    },
    {
      "metadata": {
        "id": "6tovqWRF-tjZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load data\n",
        "data_not_preprocessed = mnistutils.fetch_data()\n",
        "\n",
        "#create reprocessing functions\n",
        "input_range = [-1,1]\n",
        "preprocess, revert_preprocessing = mnistutils.create_preprocessing_f(data_not_preprocessed[0], input_range)\n",
        "\n",
        "#preprocess data\n",
        "data = (\n",
        "    preprocess(data_not_preprocessed[0]), data_not_preprocessed[1],\n",
        "    preprocess(data_not_preprocessed[2]), data_not_preprocessed[3]\n",
        ")\n",
        "\n",
        "n = 10\n",
        "test_images = list(zip(data[2][:n], data[3][:n]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pnETasf6_IrD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "The next part loads are pretained CNN on MNIST and a \"softmax-less\" model is generated."
      ]
    },
    {
      "metadata": {
        "id": "nOGIkCwv_Zfs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = load_model('models/pretrained_models/MNISTcnn.h5')\n",
        "model.get_layer(name='dense_1').name='dense_1a'\n",
        "model.get_layer(name='conv2d_1').name = 'conv2d_1a'\n",
        "model.get_layer(name='conv2d_2').name = 'conv2d_2a'\n",
        "\n",
        "#create momdel without trailing softmax\n",
        "model_wo_softmax = iutils.keras.graph.model_wo_softmax(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZEO31vEZ_wWk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup analyzers"
      ]
    },
    {
      "metadata": {
        "id": "BwP1zl8V_1Jc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "noise_scale = (input_range[1] - input_range[0]) * 0.1\n",
        "ri = input_range[0]  # reference input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LvY2q46F_8FL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Configure analysis methods and properties\n",
        "methods = [\n",
        "    # NAME                    OPT.PARAMS                POSTPROC FXN               TITLE\n",
        "\n",
        "    # Function\n",
        "    (\"gradient\", {\"postprocess\": \"abs\"}, mnistutils.graymap, \"Gradient\"),  # 0\n",
        "    (\"smoothgrad\", {\"noise_scale\": noise_scale,\n",
        "                    \"postprocess\": \"square\"}, mnistutils.graymap, \"SmoothGrad\"),  # 1\n",
        "\n",
        "    # Signal\n",
        "    (\"deconvnet\", {}, mnistutils.bk_proj, \"Deconvnet\"),  # 2\n",
        "    (\"guided_backprop\", {}, mnistutils.bk_proj, \"Guided Backprop\",),  # 3\n",
        "    (\"pattern.net\", {\"pattern_type\": \"relu\"}, mnistutils.bk_proj, \"PatternNet\"),  # 4\n",
        "\n",
        "    # Interaction\n",
        "    (\"pattern.attribution\", {\"pattern_type\": \"relu\"}, mnistutils.heatmap, \"PatternAttribution\"),  # 5\n",
        "    (\"deep_taylor.bounded\", {\"low\": input_range[0],\n",
        "                                 \"high\": input_range[1]}, mnistutils.heatmap, \"DeepTaylor\"),  # 6\n",
        "    (\"input_t_gradient\", {}, mnistutils.heatmap, \"Input * Gradient\"),  # 7\n",
        "    (\"integrated_gradients\", {\"reference_inputs\": ri}, mnistutils.heatmap, \"Integrated Gradients\"),  # 8\n",
        "    (\"lrp.z\", {}, mnistutils.heatmap, \"LRP-Z\"),  # 9\n",
        "    (\"lrp.epsilon\", {\"epsilon\": 1}, mnistutils.heatmap, \"LRP-Epsilon\"),  # 10\n",
        "    (\"lrp.epsilon_IB\", {\"epsilon\": 1}, mnistutils.heatmap, \"LRP-Epsilon\"),  # 11\n",
        "    (\"lrp.alpha_1_beta_0\", {}, mnistutils.heatmap, \"LRP-Alpha1-Beta0\"),  # 12\n",
        "    (\"lrp.alpha_1_beta_0_IB\", {}, mnistutils.heatmap, \"LRP-Alpha1-Beta0 IB\"),  # 13\n",
        "    (\"lrp.alpha_2_beta_1\", {}, mnistutils.heatmap, \"LRP-Alpha2-Beta1\"),  # 14\n",
        "    (\"lrp.alpha_2_beta_1_IB\", {}, mnistutils.heatmap, \"LRP-Alpha2-Beta1 IB\"),  # 15\n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5v0rKQK7Ae8E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select methods of your choice\n",
        "selected_methods_indices = [19]\n",
        "selected_methods = [methods[i] for i in selected_methods_indices]\n",
        "print('Using method(s) \"{}\".'.format([method[0] for method in selected_methods]))\n",
        "methods = selected_methods"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "86IZBgkgAlLt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "analyzers = []\n",
        "for method in methods:\n",
        "    analyzer = innvestigate.create_analyzer(method[0],  # analysis method identifier\n",
        "                                            model_wo_softmax,  # model without softmax output\n",
        "                                            **method[1])  # optional analysis parameters\n",
        "\n",
        "    # Some analyzers require training.\n",
        "    analyzer.fit(data[0], batch_size=256, verbose=1)\n",
        "    analyzers.append(analyzer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MVuIgFAfA61v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Translation of images"
      ]
    },
    {
      "metadata": {
        "id": "46qz-6h-A-JB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "translated_images = []\n",
        "dist = 20\n",
        "for i,(test_image, label) in enumerate(test_images):\n",
        "    test_image = np.stack((test_image,) * 3, axis=-1)\n",
        "    test_image = np.reshape(test_image,(28,28,3))\n",
        "\n",
        "    translated_images.append([])\n",
        "    tmp = []\n",
        "    M = np.float32([[1, 0, 1], [0, 1, 0]])\n",
        "    for j in range(-dist, dist + 1):\n",
        "        M[0,2] = j\n",
        "        translated_image = cv2.warpAffine(test_image, M, (28, 28))\n",
        "        translated_image = translated_image[:, :, 0]\n",
        "        translated_image = np.reshape(translated_image,(28,28,1))\n",
        "        tmp.append([translated_image,label])\n",
        "    translated_images[i] = tmp\n",
        "\n",
        "test_images = translated_images\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZdc2LX8BOYN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run analysis"
      ]
    },
    {
      "metadata": {
        "id": "iP_fOrY6BSR7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nof_test_images = len(test_images) * 2 * dist + 1\n",
        "preds = np.zeros((n,2 * dist + 1))\n",
        "analysis = np.zeros([n, 2 * dist + 1, len(analyzers), 28, 28, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UtciaZYbBaLC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for j in range(n):\n",
        "    for i, (x,y) in enumerate(test_images[j]):\n",
        "        # Add batch axis.\n",
        "        x = x[None, :, :, :]\n",
        "        \n",
        "        # Predict final activations, probabilites, and label.\n",
        "        presm = model_wo_softmax.predict_on_batch(x)[0]\n",
        "        prob = model.predict_on_batch(x)[0]\n",
        "        preds[j, i] = prob[y]\n",
        "\n",
        "        for aidx, analyzer in enumerate(analyzers):\n",
        "            # Analyze.\n",
        "            a = analyzer.analyze(x, neuron_selection=int(y))\n",
        "            \n",
        "            # Apply common postprocessing, e.g., re-ordering the channels for plotting.\n",
        "            a = mnistutils.postprocess(a)\n",
        "            analysis[j, i, aidx] = a[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XfkzbJc6BoBC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create visualization"
      ]
    },
    {
      "metadata": {
        "id": "TqFiz7JqBsoG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = list(range(-dist,dist+1))\n",
        "avg_preds = np.zeros(2*dist + 1)\n",
        "for i in range(2*dist + 1):\n",
        "    avg_preds[i] = np.average(preds[:,i])\n",
        "\n",
        "avg_analysis = np.zeros([len(analyzers), 2 * dist + 1])\n",
        "for aidx, _ in enumerate(analyzers):\n",
        "    for i in range(2 * dist + 1):\n",
        "        avg_analysis[aidx, i] = np.average(analysis[0,i, aidx, :, :, :])  # Prepare the grid as rectengular list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yvoJXEDkB2Dm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "ax1.set_xlabel('horizontal translation in pixels')\n",
        "label = \"Prediction f(x)\"\n",
        "axlegend =ax1.plot(x, avg_preds, label=label, color=\"tab:blue\")\n",
        "ax1.tick_params(axis='y', bottom=False, top=False, labelbottom=False, which='both', length=0)\n",
        "ax1.set_yticklabels([])\n",
        "\n",
        "twin_ax = []\n",
        "helper_axes = []\n",
        "linestyles = ['--', '-.', ':',(0, (5, 1)),(0, (3, 5, 1, 5)),(0, (3, 1, 1, 1, 1, 1))]\n",
        "colors = [\"green\", \"red\", \"magenta\", \"cyan\", \"brown\", \"orange\"]\n",
        "for aidx,method_name in enumerate(selected_methods):\n",
        "    twin_ax.append(ax1.twinx())\n",
        "    twin_ax[aidx].tick_params(axis='y', bottom=False, top=False, labelbottom=False, which='both', length=0)\n",
        "    twin_ax[aidx].set_yticklabels([])\n",
        "    label = \"{} R(x)\".format(method_name[3])\n",
        "    helper_axes.append(twin_ax[aidx].plot(x, avg_analysis[aidx], label=label, linestyle=linestyles[aidx], color=colors[aidx]))\n",
        "\n",
        "lns = axlegend\n",
        "for tmp in helper_axes:\n",
        "   lns = lns + tmp\n",
        "labs = [l.get_label() for l in lns]\n",
        "ax1.legend(lns, labs)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}